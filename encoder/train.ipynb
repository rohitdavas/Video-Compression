{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports and args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:43:08.369698Z",
     "start_time": "2020-02-19T18:43:08.198771Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as LS\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from dataset import get_loader\n",
    "from evaluate import run_eval\n",
    "from train_options import parser\n",
    "from util import get_models, init_lstm, set_train, set_eval\n",
    "from util import prepare_inputs, forward_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:45:50.852213Z",
     "start_time": "2020-02-19T18:45:50.845094Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --train TRAIN --eval EVAL\n",
      "                             [--distance1 DISTANCE1] [--distance2 DISTANCE2]\n",
      "                             [--train-mv TRAIN_MV] [--eval-mv EVAL_MV]\n",
      "                             [--v-compress] [--iterations ITERATIONS]\n",
      "                             [--bits BITS] [--patch PATCH] [--shrink SHRINK]\n",
      "                             [--warp] [--fuse-encoder]\n",
      "                             [--encoder-fuse-level ENCODER_FUSE_LEVEL]\n",
      "                             [--decoder-fuse-level DECODER_FUSE_LEVEL]\n",
      "                             [--stack] [--max-train-iters MAX_TRAIN_ITERS]\n",
      "                             [--lr LR] [--clip CLIP] [--schedule SCHEDULE]\n",
      "                             [--gamma GAMMA] [--batch-size BATCH_SIZE]\n",
      "                             [--eval-batch-size EVAL_BATCH_SIZE]\n",
      "                             [--num-crops NUM_CROPS] [--gpus GPUS]\n",
      "                             [--out-dir OUT_DIR] [--model-dir MODEL_DIR]\n",
      "                             [--load-model-name LOAD_MODEL_NAME]\n",
      "                             [--load-iter LOAD_ITER]\n",
      "                             [--save-model-name SAVE_MODEL_NAME]\n",
      "                             [--save-codes] [--save-out-img]\n",
      "                             [--checkpoint-iters CHECKPOINT_ITERS]\n",
      "                             [--eval-iters EVAL_ITERS]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --train, --eval\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3339: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "'''args'''\n",
    "args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:46:19.933378Z",
     "start_time": "2020-02-19T18:46:19.906131Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1a2d8ca318b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m'''train'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m train_loader = get_loader(is_train=True,\n\u001b[0;32m----> 3\u001b[0;31m                           \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                           \u001b[0mmv_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                           args=args)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "'''train'''\n",
    "train_loader = get_loader(is_train=True,\n",
    "                          root=args.train,\n",
    "                          mv_dir=args.train_mv,\n",
    "                          args=args)\n",
    "'''eval'''\n",
    "\n",
    "\n",
    "def get_eval_loaders():\n",
    "    # We can extend this dict to evaluate on multiple datasets.\n",
    "    eval_loaders = {\n",
    "        'TVL':\n",
    "        get_loader(is_train=False,\n",
    "                   root=args.eval,\n",
    "                   mv_dir=args.eval_mv,\n",
    "                   args=args),\n",
    "    }\n",
    "    return eval_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, binarizer, decoder, unet = get_models(\n",
    "    args=args,\n",
    "    v_compress=args.v_compress,\n",
    "    bits=args.bits,\n",
    "    encoder_fuse_level=args.encoder_fuse_level,\n",
    "    decoder_fuse_level=args.decoder_fuse_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = [encoder, binarizer, decoder]\n",
    "if unet is not None:\n",
    "    nets.append(unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Gpu setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''gpu setup'''\n",
    "gpus = [int(gpu) for gpu in args.gpus.split(',')]\n",
    "if len(gpus) > 1:\n",
    "    print(\"Using GPUs {}.\".format(gpus))\n",
    "    for net in nets:\n",
    "        net = nn.DataParallel(net, device_ids=gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [{'params': net.parameters()} for net in nets]\n",
    "\n",
    "solver = optim.Adam(\n",
    "    params,\n",
    "    lr=args.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "milestones = [int(s) for s in args.schedule.split(',')]\n",
    "scheduler = LS.MultiStepLR(solver, milestones=milestones, gamma=args.gamma)\n",
    "\n",
    "if not os.path.exists(args.model_dir):\n",
    "    print(\"Creating directory %s.\" % args.model_dir)\n",
    "    os.makedirs(args.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## checkpoint create and resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     13
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def resume(index):\n",
    "    names = ['encoder', 'binarizer', 'decoder', 'unet']\n",
    "\n",
    "    for net_idx, net in enumerate(nets):\n",
    "        if net is not None:\n",
    "            name = names[net_idx]\n",
    "            checkpoint_path = '{}/{}_{}_{:08d}.pth'.format(\n",
    "                args.model_dir, args.save_model_name, name, index)\n",
    "\n",
    "            print('Loading %s from %s...' % (name, checkpoint_path))\n",
    "            net.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "\n",
    "def save(index):\n",
    "    names = ['encoder', 'binarizer', 'decoder', 'unet']\n",
    "\n",
    "    for net_idx, net in enumerate(nets):\n",
    "        if net is not None:\n",
    "            torch.save(\n",
    "                encoder.state_dict(),\n",
    "                '{}/{}_{}_{:08d}.pth'.format(args.model_dir,\n",
    "                                             args.save_model_name,\n",
    "                                             names[net_idx], index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = 0\n",
    "just_resumed = False\n",
    "if args.load_model_name:\n",
    "    print('Loading %s@iter %d' % (args.load_model_name,\n",
    "                                  args.load_iter))\n",
    "\n",
    "    resume(args.load_model_name, args.load_iter)\n",
    "    train_iter = args.load_iter\n",
    "    scheduler.last_epoch = train_iter - 1\n",
    "    just_resumed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     23,
     25,
     29,
     40,
     42,
     46,
     54,
     68,
     76,
     86,
     89,
     115
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    for batch, (crops, ctx_frames, _) in enumerate(train_loader):\n",
    "        scheduler.step()\n",
    "        train_iter += 1\n",
    "\n",
    "        if train_iter > args.max_train_iters:\n",
    "            break\n",
    "\n",
    "        batch_t0 = time.time()\n",
    "\n",
    "        solver.zero_grad()\n",
    "\n",
    "        # Init LSTM states.\n",
    "        (encoder_h_1, encoder_h_2, encoder_h_3, decoder_h_1, decoder_h_2,\n",
    "         decoder_h_3, decoder_h_4) = init_lstm(\n",
    "                                               batch_size=(crops[0].size(0) *args.num_crops),\n",
    "                                               height=crops[0].size(2),\n",
    "                                               width=crops[0].size(3),\n",
    "                                               args=args\n",
    "                                               )\n",
    "\n",
    "        # Forward U-net.\n",
    "        if args.v_compress:\n",
    "            unet_output1, unet_output2 = forward_ctx(unet, ctx_frames)\n",
    "        else:\n",
    "            unet_output1 = Variable(torch.zeros(args.batch_size, )).cuda()\n",
    "            unet_output2 = Variable(torch.zeros(args.batch_size, )).cuda()\n",
    "\n",
    "        res, frame1, frame2, warped_unet_output1, warped_unet_output2 = prepare_inputs(\n",
    "            crops, args, unet_output1, unet_output2)\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        bp_t0 = time.time()\n",
    "        _, _, height, width = res.size()\n",
    "\n",
    "        out_img = torch.zeros(1, 3, height, width).cuda() + 0.5\n",
    "\n",
    "        for _ in range(args.iterations):\n",
    "            if args.v_compress and args.stack:\n",
    "                encoder_input = torch.cat([frame1, res, frame2], dim=1)\n",
    "            else:\n",
    "                encoder_input = res\n",
    "\n",
    "            # Encode.\n",
    "            encoded, encoder_h_1, encoder_h_2, encoder_h_3 = encoder(\n",
    "                encoder_input, encoder_h_1, encoder_h_2, encoder_h_3,\n",
    "                warped_unet_output1, warped_unet_output2)\n",
    "\n",
    "            # Binarize.\n",
    "            codes = binarizer(encoded)\n",
    "\n",
    "            # Decode.\n",
    "            (output, decoder_h_1, decoder_h_2, decoder_h_3,\n",
    "             decoder_h_4) = decoder(codes, decoder_h_1, decoder_h_2,\n",
    "                                    decoder_h_3, decoder_h_4,\n",
    "                                    warped_unet_output1, warped_unet_output2)\n",
    "\n",
    "            res = res - output\n",
    "            out_img = out_img + output.data\n",
    "            losses.append(res.abs().mean())\n",
    "\n",
    "        bp_t1 = time.time()\n",
    "\n",
    "        loss = sum(losses) / args.iterations\n",
    "        loss.backward()\n",
    "\n",
    "        for net in [encoder, binarizer, decoder, unet]:\n",
    "            if net is not None:\n",
    "                torch.nn.utils.clip_grad_norm(net.parameters(), args.clip)\n",
    "\n",
    "        solver.step()\n",
    "\n",
    "        batch_t1 = time.time()\n",
    "\n",
    "        print(\n",
    "            '[TRAIN] Iter[{}]; LR: {}; Loss: {:.6f}; Backprop: {:.4f} sec; Batch: {:.4f} sec'\n",
    "            .format(train_iter,\n",
    "                    scheduler.get_lr()[0], loss.item(), bp_t1 - bp_t0,\n",
    "                    batch_t1 - batch_t0))\n",
    "\n",
    "        # if train_iter % 100 == 0:\n",
    "        #   print('Loss at each step:')\n",
    "        #  print(('{:.4f} ' * args.iterations +\"\\n\").format(* [l.item() for l in losses]))\n",
    "\n",
    "        if train_iter % args.checkpoint_iters == 0:\n",
    "            save(train_iter)\n",
    "\n",
    "        if just_resumed or train_iter % args.eval_iters == 0 or train_iter == 100:\n",
    "            print('Start evaluation...')\n",
    "\n",
    "            set_eval(nets)\n",
    "\n",
    "            eval_loaders = get_eval_loaders()\n",
    "            for eval_name, eval_loader in eval_loaders.items():\n",
    "                eval_begin = time.time()\n",
    "                eval_loss, mssim, psnr = run_eval(nets,\n",
    "                                                  eval_loader,\n",
    "                                                  args,\n",
    "                                                  output_suffix='iter%d' %\n",
    "                                                  train_iter)\n",
    "\n",
    "                print('Evaluation @iter %d done in %d secs' %\n",
    "                      (train_iter, time.time() - eval_begin))\n",
    "                print('%s Loss   : ' % eval_name +\n",
    "                      '\\t'.join(['%.5f' % el for el in eval_loss.tolist()]))\n",
    "                print('%s MS-SSIM: ' % eval_name +\n",
    "                      '\\t'.join(['%.5f' % el for el in mssim.tolist()]))\n",
    "                print('%s PSNR   : ' % eval_name +\n",
    "                      '\\t'.join(['%.5f' % el for el in psnr.tolist()]))\n",
    "\n",
    "            set_train(nets)\n",
    "            just_resumed = False\n",
    "\n",
    "    if train_iter > args.max_train_iters:\n",
    "        print('Training done.')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
